{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Automatic scoring of x-ray images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# PIL to open & manipulate images\n",
    "from PIL import Image, ImageOps, ImageChops\n",
    "\n",
    "# for messages in loops\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# to save arrays\n",
    "import h5py\n",
    "\n",
    "# for folder-timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "# for train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# for one-hot encoding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# for class weights\n",
    "from sklearn.utils import class_weight\n",
    "# for model evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# for efficient loops\n",
    "import itertools\n",
    "\n",
    "# keras\n",
    "from tensorflow.contrib.keras.python.keras import backend as K\n",
    "from tensorflow.contrib.keras.python.keras.utils.io_utils import HDF5Matrix\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Lambda, Activation\n",
    "from tensorflow.contrib.keras.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.contrib.keras.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.contrib.keras.python.keras import callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define image format & random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image format -> (rows, cols, channels)\n",
    "K.set_image_data_format(\"channels_last\")\n",
    "# fix random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/img_train_regression.h5', 'r') as hf:\n",
    "    img_train = hf['img_train_regression'][:]\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train_regression.h5', 'r') as hf:\n",
    "    labels_train = hf['labels_train_regression'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=25,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=[0,1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def round_x(x, base=5):\n",
    "    return int(base * round(float(x)/base))\n",
    "    \n",
    "labels_train_rounded = [ round_x(x) for x in labels_train ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85\n",
      "  90  95 100]\n",
      "[48855 11547  5418  1376  1657   414   634   179   469   114   317    59\n",
      "   236    54   160    25   154    31   125     9   792]\n"
     ]
    }
   ],
   "source": [
    "unique_numbers, n_img = np.unique(ar=labels_train_rounded, return_counts=True)\n",
    "print(unique_numbers)\n",
    "print(n_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137375\n"
     ]
    }
   ],
   "source": [
    "print(sum(10000 - n_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.02577386  0.04851021  0.04692958  0.05392149\n",
      "  0.05268398  0.05524337  0.05361211  0.05560899  0.05446711  0.05591837\n",
      "  0.05492274  0.05594649  0.05535024  0.05610962  0.05538399  0.05607587\n",
      "  0.05554712  0.05619962  0.05179523]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "tmp = 10000 - n_img\n",
    "tmp = [max(0, x) for x in tmp]\n",
    "probs = tmp / sum(tmp)\n",
    "print(probs)\n",
    "print(len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data, 100.00 % finished\n",
      "(72625, 150, 150, 1)\n",
      "(137400, 1, 150, 150, 1)\n",
      "(72625,)\n",
      "(137400, 1)\n"
     ]
    }
   ],
   "source": [
    "img_train_augmented = []\n",
    "labels_train_augmented = []\n",
    "\n",
    "n_augmented = 137400\n",
    "b_size = 1\n",
    "\n",
    "for i in range(n_augmented):\n",
    "    clear_output()\n",
    "    print(\"Augmenting data, {0:.2f} % finished\".format(i/n_augmented*100))\n",
    "    lbl = np.random.choice(a=unique_numbers, p=probs)\n",
    "    for batch in datagen.flow(img_train[labels_train_rounded == lbl], batch_size=b_size):\n",
    "        img_train_augmented.append(batch)\n",
    "        labels_train_augmented.append(np.repeat(lbl,b_size))\n",
    "        break\n",
    "\n",
    "img_train_augmented = np.array(img_train_augmented)\n",
    "\n",
    "labels_train_augmented = np.array(labels_train_augmented)\n",
    "\n",
    "print(img_train.shape)\n",
    "print(img_train_augmented.shape)\n",
    "print(labels_train.shape)\n",
    "print(labels_train_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/img_train_augmented_regression.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"img_train_augmented_regression\",  data=img_train_augmented)\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train_augmented_regression.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"labels_train_augmented_regression\",  data=labels_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/img_train_augmented_regression.h5', 'r') as hf:\n",
    "    img_train_augmented = hf['img_train_augmented_regression'][:]\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train_augmented_regression.h5', 'r') as hf:\n",
    "    labels_train_augmented = hf['labels_train_augmented_regression'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72625, 150, 150, 1)\n",
      "(137400, 150, 150, 1)\n",
      "(72625,)\n",
      "(137400,)\n"
     ]
    }
   ],
   "source": [
    "img_train_augmented = img_train_augmented.reshape(img_train_augmented.shape[0] * img_train_augmented.shape[1], \n",
    "                                                  img_train_augmented.shape[2], img_train_augmented.shape[3], \n",
    "                                                  img_train_augmented.shape[4])\n",
    "labels_train_augmented = labels_train_augmented.reshape(labels_train_augmented.shape[0] * \n",
    "                                                        labels_train_augmented.shape[1])\n",
    "\n",
    "print(img_train.shape)\n",
    "print(img_train_augmented.shape)\n",
    "print(labels_train.shape)\n",
    "print(labels_train_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_train_combined = np.concatenate([img_train, img_train_augmented])\n",
    "labels_train_combined = np.concatenate([labels_train, labels_train_augmented])\n",
    "\n",
    "idx = np.random.permutation(len(img_train_combined))\n",
    "img_train_combined = img_train_combined[idx]\n",
    "labels_train_combined = labels_train_combined[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/img_train_combined_regression.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"img_train_combined_regression\",  data=img_train_combined)\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train_combined_regression.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"labels_train_combined_regression\",  data=labels_train_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
