{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Automatic scoring of x-ray images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# PIL to open & manipulate images\n",
    "from PIL import Image, ImageOps, ImageChops\n",
    "\n",
    "# for messages in loops\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# to save arrays\n",
    "import h5py\n",
    "\n",
    "# for folder-timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "# for train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# for one-hot encoding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# for class weights\n",
    "from sklearn.utils import class_weight\n",
    "# for model evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# for efficient loops\n",
    "import itertools\n",
    "\n",
    "# keras\n",
    "from tensorflow.contrib.keras.python.keras import backend as K\n",
    "from tensorflow.contrib.keras.python.keras.utils.io_utils import HDF5Matrix\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Lambda, Activation\n",
    "from tensorflow.contrib.keras.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.contrib.keras.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.contrib.keras.python.keras import callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define image format & random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image format -> (rows, cols, channels)\n",
    "K.set_image_data_format(\"channels_last\")\n",
    "# fix random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/img_train.h5', 'r') as hf:\n",
    "    img_train = hf['img_train'][:]\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train.h5', 'r') as hf:\n",
    "    labels_train = hf['labels_train'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=25,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=[0,1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48820 19676  1638   754   435   975]\n"
     ]
    }
   ],
   "source": [
    "n_img = np.unique(ar=labels_train, return_counts=True)[1]\n",
    "print(n_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227702\n"
     ]
    }
   ],
   "source": [
    "print(sum(50000 - n_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00518221  0.13317406  0.21239163  0.2162739   0.21767486  0.21530334]\n"
     ]
    }
   ],
   "source": [
    "tmp = 50000 - n_img\n",
    "probs = tmp / sum(tmp)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data, 99.96 % finished\n"
     ]
    }
   ],
   "source": [
    "img_train_augmented = []\n",
    "labels_train_augmented = []\n",
    "\n",
    "n_augmented = 2277\n",
    "b_size = 100\n",
    "\n",
    "for i in range(n_augmented):\n",
    "    clear_output()\n",
    "    print(\"Augmenting data, {0:.2f} % finished\".format(i/n_augmented*100))\n",
    "    lbl = np.random.choice(a=6, p=probs)\n",
    "    for batch in datagen.flow(img_train[labels_train == lbl], batch_size=b_size):\n",
    "        img_train_augmented.append(batch)\n",
    "        labels_train_augmented.append(np.repeat(lbl,b_size))\n",
    "        break\n",
    "\n",
    "img_train_augmented = np.array(img_train_augmented)\n",
    "\n",
    "labels_train_augmented = np.array(labels_train_augmented)\n",
    "\n",
    "print(img_train.shape)\n",
    "print(img_train_augmented.shape)\n",
    "print(labels_train.shape)\n",
    "print(labels_train_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/img_train_augmented.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"img_train_augmented\",  data=img_train_augmented)\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train_augmented.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"labels_train_augmented\",  data=labels_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/img_train_augmented.h5', 'r') as hf:\n",
    "    img_train_augmented = hf['img_train_augmented'][:]\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train_augmented.h5', 'r') as hf:\n",
    "    labels_train_augmented = hf['labels_train_augmented'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72298, 150, 150, 1)\n",
      "(227700, 150, 150, 1)\n",
      "(72298,)\n",
      "(227700,)\n"
     ]
    }
   ],
   "source": [
    "img_train_augmented = img_train_augmented.reshape(img_train_augmented.shape[0] * img_train_augmented.shape[1], \n",
    "                                                  img_train_augmented.shape[2], img_train_augmented.shape[3], \n",
    "                                                  img_train_augmented.shape[4])\n",
    "labels_train_augmented = labels_train_augmented.reshape(labels_train_augmented.shape[0] * \n",
    "                                                        labels_train_augmented.shape[1])\n",
    "\n",
    "print(img_train.shape)\n",
    "print(img_train_augmented.shape)\n",
    "print(labels_train.shape)\n",
    "print(labels_train_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_train_combined = np.concatenate([img_train, img_train_augmented])\n",
    "labels_train_combined = np.concatenate([labels_train, labels_train_augmented])\n",
    "\n",
    "idx = np.random.permutation(len(img_train_combined))\n",
    "img_train_combined = img_train_combined[idx]\n",
    "labels_train_combined = labels_train_combined[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/img_train_combined.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"img_train_combined\",  data=img_train_combined)\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train_combined.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"labels_train_combined\",  data=labels_train_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/data/joint_scoring/labels_train_combined.h5', 'r') as hf:\n",
    "    labels_train_combined = hf['labels_train_combined'][:]\n",
    "\n",
    "# one hot encode outputs\n",
    "labels_train_combined_onehot = LabelBinarizer().fit_transform(labels_train_combined)\n",
    "\n",
    "with h5py.File('/data/joint_scoring/labels_train_combined_onehot.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"labels_train_combined_onehot\",  data=labels_train_combined_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
